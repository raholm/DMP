\documentclass[report.tex]{subfiles}

\begin{document}
    \section*{\centering Method}

    In order the answer the research questions presented in the introduction, we have divided the experiments into two parts. One experiments in which we look at the correlation between the reward function and the goal of the game and another where we investigate the performance of different state representations.

    \subsection*{Reward Experiment}

    Our hypothesis is that it is essential that the reward function reflects what the goal of the problem is and in our case that is to gather as many apples as possible, preferably in fewer time steps. To perform this experiments we have constructed 9 reward functions with all the combinations of the following characteristics

    \begin{center}
        \begin{tabular}{| l |}
            \hline
            \textbf{Reward at the end of the episode} \\ \hline
            $\pm$ score or 0 \\ \hline
            \textbf{Reward after each time step} \\ \hline
            $\pm 1$ or 0 \\ \hline
        \end{tabular}
    \end{center}

    The reasoning for the construction of these particular reward functions is the answer the research questions.

    \textbf{Is it important for the reward function to reinforce behavior we want the agent to learn?} If this is true we expect the reward functions that gives positive score rewards regardless of reward per time step to have the best performances.

    \textbf{Is punishing bad behavior equivalent to reinforcing good behavior?} If this is true we expect the reward function that gives 0 rewards at the end of the episode and -1 reward each time step has the similar performance to the ones hypothesised as the best in the previous question.

    \textbf{Is it better if the reward function combines reinforcement and punishment?} If this is true we expect the reward function that gives positive score rewards at the end of the episode and -1 reward at each time step has the best performance.

    In order to do this experiments we also need to choose state representations and we choose to use two very different ones. The board state with the game score and the directional state with the game score, as described below.

    \subsection*{State Experiment}

    In order to determine the importance of the the state representation we have decided upon 4 different representations. Additionally, we have decided to test whether it is important for the state to contain the board dimensions to help the agent to infer deadly state-action pairs. And also if the current game score is important for the agent to be able to predict the future rewards more accurately.

    Intuitively, it would be reasonable to assume that if the agent knows the dimensions of the board it could more easily infer state-action pairs that result in a terminal state and thus end the game with potentially less total reward.

    By the same reasoning is it probably a good idea to give the agent access to its current score for it to improve the estimation of future rewards and thus making better decisions.

    The 4 state representations are the following

    \begin{itemize}
        \item \textbf{Board}: Represents the complete board state as the game engine represents it.
        \item \textbf{SnakeFood}: Represents coordinates of the snake's head/body and the coordinate of the food source.
        \item \textbf{Directional}: Represents the direction the snake should travel to get to the food source without following the dynamics of the game. That means it might not be possible to change to that direction with a single action, e.g., the snake travels east but should travel west.
        \item \textbf{ShortestPath}: Represents the shortest path from the head of the snake to the apple.
    \end{itemize}

    and each state is split up into 4 different states considering with/without board dimensions and with/without score, which gives 16 states in total.

    To perform this experiments we also require a reward function which was decided based on the reward experiments by picking the best performing one. It turned out to be the reward function that gave 0 reward each time step and positive score as reward at the end of the episode.

    \subsubsection*{Hyperparameters}

    For all the experiments we have used the following hyperparameters based on trial-and-error experimentation

    \begin{itemize}
        \item $\epsilon$-greedy policy with exploration rate $\epsilon = 0.15$
        \item Training for 1 million episodes
        \item Initialized the Q-function with 0s
    \end{itemize}

    \begin{center}
        \begin{tabular}{| c | c |}
            \hline
            Learning rate $\alpha$ & Discount factor $\gamma$ \\ \hline
            \multicolumn{2}{ |c| }{Q-Learning} \\ \hline
            0.85 & 0.85 \\ \hline
            \multicolumn{2}{ |c| }{Sarsa} \\ \hline
            0.15 & 0.95 \\ \hline
            \multicolumn{2}{ |c| }{Expected Sarsa} \\ \hline
            0.15 & 0.95 \\ \hline
        \end{tabular}
    \end{center}

\end{document}
