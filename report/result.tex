\documentclass[result.tex]{subfiles}

\begin{document}

    \section*{\centering Result}

    In this part of the report we will present the result that have been gathered from our two experiments on reward functions and state representations. It will be presented in the order aforementioned.

    \subsection*{Reward Experiment}

    In this section we will present the results given by the reward experiment. The upper plots have used the board state representation and the lower plots have used the directional state. All the results have been averaged over 3 different experiments.

    \begin{figure}[ht]
        \centering
        \begin{subfigure}[b]{0.8\linewidth}
            \includegraphics[width=\linewidth]{../images/qlearning/reward/42/reward_qlearning_board_state_average_game_score_over_time.png}
        \end{subfigure}

        \begin{subfigure}[b]{0.8\linewidth}
            \includegraphics[width=\linewidth]{../images/qlearning/reward/42/reward_qlearning_directional_state_average_game_score_over_time.png}
        \end{subfigure}
        \caption{Results generated by Q-Learning. Upper: Board state. Lower: Directional State.}
        \label{fig:reward_result_qlearning}
    \end{figure}

    \newpage

    \begin{figure}[ht]
        \centering
        \begin{subfigure}[b]{.8\linewidth}
            \includegraphics[width=\linewidth]{../images/sarsa/reward/42/reward_sarsa_board_state_average_game_score_over_time.png}
        \end{subfigure}

        \begin{subfigure}[b]{.8\linewidth}
            \includegraphics[width=\linewidth]{../images/sarsa/reward/42/reward_sarsa_directional_state_average_game_score_over_time.png}
        \end{subfigure}
        \caption{Results generated by Sarsa. Upper: Board state. Lower: Directional State.}
        \label{fig:reward_result_sarsa}
    \end{figure}

    \newpage

    The plots show the average game score the agent learns to score on average which is the goal to maximize. What we mean by \enquote{average over time} is that we take the average of all data points up to (inclusive) the time we are interested in, i.e., a moving average with an infinite tail. In our case time refers to the number of episodes we have trained the model.

    We also looked at the correlation between the rewards over time (not shown) and the game score curves above and can be read off in table \ref{tab:reward_game_score_corr}.

    \begin {table}[H]
        \centering
        \begin{tabular}{c|c|}
            \cline{2-2}
            & \multicolumn{1}{ c| }{Avg. Correlation} \\
            \cline{2-2}
            & \multicolumn{1}{ c| }{Q-Learning} \\
            \cline{1-2}
            \multicolumn{1}{ |c|  }{Board State}
            & 0.95 \\
            \cline{1-2}
            \multicolumn{1}{ |c|  }{Directional State}
            & 0.84 \\
            \cline{1-2}
            & \multicolumn{1}{ c| }{Sarsa} \\
            \cline{1-2}
            \multicolumn{1}{ |c|  }{Board State}
            & 0.97 \\
            \cline{1-2}
            \multicolumn{1}{ |c|  }{Directional State}
            & 0.91 \\
            \cline{1-2}
        \end{tabular}
        \caption {The average correlation between the reward functions and game score.}
        \label{tab:reward_game_score_corr}
    \end{table}

    \newpage

    \subsubsection*{State Experiment}

    In this section we present the results from the state experiment. For each algorithm we have looked at the 16 states presented in the method. Since the goal is to look at the objective of the problem we plot the average game score each state gets over 1 million episodes of training. Same as before we have averaged the result over 3 experiments.

    The first thing we looked at was augmenting the state representations with board dimensions and game score. Figure \ref{fig:info_augmentation_board_state} shows the results using the board representation and trained with Q-learning. We saw similar results for other states and algorithms.

    \begin{figure}[ht]
        \centering
        \includegraphics[width=\linewidth]{../images/qlearning/info_augmentation/123/board_state_average_game_score_over_time.png}
        \caption{Board state with different augmented information.}
        \label{fig:info_augmentation_board_state}
    \end{figure}

    \newpage

    We trained the models on a lot of different states as stated in the method and in figure \ref{fig:state_qlearning} shows the most promising states trained with Q-learning. We have included test performance in table \ref{table:state_qlearning} by averaging scores over 10000 episodes (games) using the greedy policy without exploration ($\epsilon = 0$). It also shows the performance of an agent that chooses action uniformly at random as a comparison of what we would expect from a terrible model.

    \begin{figure}[ht]
        \centering
        \includegraphics[width=\linewidth]{../images/qlearning/state/42/state_qlearning_average_game_score_over_time.png}
        \caption{Performance of state representations trained using Q-learning.}
        \label{fig:state_qlearning}
    \end{figure}

    \begin{table}[ht]
        \centering
        \begin{tabular}{ | l | l | l | }
            \hline
            State & Avg. Score & Std. Deviation \\ \hline
            ShortestPathScoreState & 129.50 & 51.11 \\ \hline
            BoardScoreState & 103.12 & 17.73 \\ \hline
            DirectionalScoreState & 66.12 & 64.328 \\ \hline
            SnakeFoodScoreState & 104.12 & 20.08 \\ \hline
            Random Agent & 20.13 & 46.47 \\
            \hline
        \end{tabular}
        \caption{Test performance over 10000 episodes.}
        \label{table:state_qlearning}
    \end{table}

    \newpage

    Similar results as previously are shown in figure \ref{fig:state_sarsa} and table \ref{table:state_sarsa} but trained with Sarsa. We have not included any results for Expected Sarsa since its performance and behavior was very similar to Sarsa. The appendix at the end includes more results on other states that was less promising in terms of game score performance.

    \begin{figure}[ht]
        \centering
        \includegraphics[width=\linewidth]{../images/sarsa/state/42/state_sarsa_average_game_score_over_time.png}
        \caption{Performance of state representations trained using Sarsa.}
        \label{fig:state_sarsa}
    \end{figure}

    \begin{table}[ht]
        \centering
        \begin{tabular}{ | l | l | l | }
            \hline
            State & Avg. Score & Std. Deviation \\ \hline
            ShortestPathScoreState & 118.59 & 38.95 \\ \hline
            BoardScoreState & 101.02 & 10.05 \\ \hline
            DirectionalScoreState & 72.16 & 71.60 \\ \hline
            SnakeFoodScoreState & 101.02 & 10.05 \\
            Random Agent & 20.13 & 46.47 \\
            \hline
        \end{tabular}
        \caption{Test performance over 10000 episodes.}
        \label{table:state_sarsa}
    \end{table}

\end{document}
