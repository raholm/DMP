\documentclass[result.tex]{subfiles}

\begin{document}

    \section*{\centering Theory}

    \subsection*{Markov Decision Process}

    \subsubsection*{Dynamics}

    \begin{align*}
        p(\nextstate, r | s, a) &=
        Pr(S_{t + 1} = \nextstate, R_{t + 1} = r | S_t = s, A_t = a) \\
        p(\nextstate | s, a) &=
        Pr(S_{t + 1} = \nextstate| S_t = s, A_t = a) =
        \sum_r p(\nextstate, r | s, a) \\
        \pi(a | s) &= Pr(A_t = a | S_t = s)
    \end{align*}

    \subsubsection*{Expected Reward}

    \begin{align*}
        r(s, a) &=
        \mathbb{E} \left[ R_{t + 1} \bigg\rvert S_t = s, A_t = a \right] =
        \sum_r r \sum_{\nextstate} p(\nextstate, r | s, a) \\
        r(s, a, \nextstate) &=
        \mathbb{E} \left[ R_{t + 1} \bigg\rvert S_t = s, A_t = a, S_{t + 1} = \nextstate \right] =
        \frac{\sum_r r p(\nextstate, r | s, a)}{p(\nextstate | s, a)}
    \end{align*}


    \subsubsection*{State-Value Function}

    \begin{align*}
        v_{\pi}(s) &= \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1} \bigg\rvert S_t = s \right] \\
        &= \sum_a \pi(a | s) \sum_{\nextstate, r} p(\nextstate, r | s, a) \left[ r + \gamma v_\pi(\nextstate) \right]
    \end{align*}

    \subsubsection*{Action-Value Function}

    \begin{align*}
        q_{\pi}(s, a) = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t + k + 1} \bigg\rvert S_t = s, A_t = a \right]
    \end{align*}

    \subsection*{Temporal Difference Learning}

    \textit{Temporal Difference} (TD) learning is a learning methodology that learns directly from raw experience without a model of the environment's dynamics. It update new estimates partially on previous estimates without waiting for the final outcome, i.e. bootstraps estimates. The algorithms, or learning methods, that have been used in this paper are all based on TD learning and will be described below.

    \subsubsection*{Q-Learning}

    Q-learning is a widely used algorithm that was developed by \textbf{(Watkins, 1989)} and is heavily used to day combined with the emergence of neural networks \textbf{deep reinforcement learning, atari}. It is defined as

    \begin{align*}
        Q(S_t, A_t) &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma \max_a Q(S_{t + 1}, a) - Q(S_t, A_t).
        \right]
    \end{align*}

    It is a so called \textit{off-policy} method which is indicated by the $\max_a$ operation. That means it uses a greedy algorithm to estimate the return at the next step rather than following its current policy, therefore is off its own policy. The Sarsa algorithm described below is an example of a on-policy method.

    \subsubsection*{Sarsa}

    \begin{align*}
        Q(S_t, A_t) &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma Q(S_{t + 1}, A_{t + 1}) - Q(S_t, A_t)
        \right]
    \end{align*}

    \subsubsection*{Expected Sarsa}

    \begin{align*}
        Q(S_t, A_t) &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma \mathbb{E} \left[ Q(S_{t + 1}, A_{t + 1}) | S_{t + 1}
        \right] -
        Q(S_t, A_t)
        \right] \\
        &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma \sum_a \pi (a | S_{t + 1}) Q(S_{t + 1}, a) -
        Q(S_t, A_t)
        \right]
    \end{align*}

    \subsection*{Other Learning Methods}

    \subsubsection*{Dynamic Programming}

    \subsubsection*{Monte Carlo}

    \subsubsection*{Extensions}

\end{document}
