\documentclass[result.tex]{subfiles}

\begin{document}

    \section*{\centering Theory}

    In this chapter we will formalize the reinforcement learning problem mathematically and define the methods to solve our particular reinforcement learning problem.

    \subsection*{Framework}

    To begin the definition of the reinforcement learning problem we first need to define a few fundamental terms that are required in the following formalization. The basic idea is to learn from interactions in order to achieve a goal. The learner is called the \textit{agent} which interacts with the \textit{environment} by performing \textit{actions} in a sequential manner. The agent is continuously interacting with the environment and the environment in turn responds based on those interactions in the form of a representation of its current \textit{state}. Not only may the environment change its state in reponse to the agent's actions, it also give rise to \textit{rewards}, numerical values that the agent is trying to maximize through its actions. Figure \ref{fig:agent_env} shows the interaction cycle between the agent and the environment. A complete specification of an environment and how its rewards are determined defines a \textit{task}, which is an instance of a reinforcement learning problem.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{./images/agent_env}
        \caption{The agent-environment interaction loop.}
        \label{fig:agent_env}
    \end{figure}

    This rather simple framework is actually abstract and flexible enough to be applied in many problems in many different ways. The actions could range from low level controls such as turning the wheels on the car to driving from point A to point B. Similarly, the state could represent current road surface condition to traffic flow in the nearby area. The reward will however always be a single numerical value since we require it to be comparable and have a well defined ordering to be able to determine the best action, the action that will maximize the total amount of reward the agent receives. We will come back to what that means in the following sections.

    \subsection*{Snake}

    Before defining the \textit{Markov decision process} we present our problem we want our agent to try to solve which is the game \textit{Snake}\footnote{https://en.wikipedia.org/wiki/Snake\_(video\_game)} and its an old game with many variants. In this report, the game is played out as simple as possible with only one player, i.e., the agent and no obstructing environment. The goal of the game is that you play as a snake by controlling the head and the aim is to eat as many apples as possible without colliding with either the snake's body or the edges of the game board. At each time unit the snake moves one unit in the direction it is facing and the difficulty is that each time you eat an apple your body becomes longer by 1 unit and thus larger portion of the game board is covered by it. We have defined the final game score to be 100 per eaten apple.  In order to reduce the state space the game board is played on a 5x5 grid which can be seen in figure \ref{fig:snake}.

    \begin{figure}[H]
        \centering
        \includegraphics[width=.3\textwidth]{./images/snake1}\hfill
        \includegraphics[width=.3\textwidth]{./images/snake2}\hfill
        \includegraphics[width=.3\textwidth]{./images/snake3}
        \caption{5x5 board of Snake where the snake consists of the orange (head) and green parts (body) and the red part is the apple.}
        \label{fig:snake}
    \end{figure}

    We will use Snake to further concretize the definitions below.

    \subsection*{Markov Decision Process}

    We are going to limit our problem formalization to a special case with discrete time steps, finite state space, finite action space, fully observable state, and episodic tasks. It can be generalized to continuous time steps, infinite state space, infinite action space, partially observable state, and continuing tasks, but it is out of scope for this report. Bertsekas and Tsitsiklis give an in-depth coverage of reinforcement learning, also known as optimal control, in \textit{Neuro-dynamic programming} \cite{bertsekas1995neuro}

    \subsubsection*{Notation}

    Before continuing with the definition we need to present the most important notation that will be used from now on through the rest of the report.
    \newline
    $\actionspace$: Action space \\
    $\statespace$: State space \\
    $A_t \in \actionspace$: Action at time $t$ \\
    $S_t \in \statespace$: State at time $t$ \\
    $R_t \in \rewardspace$: Reward at time $t$
    \newline
    where $t \in \{ 0, 1, 2, \ldots, T \}$ since we have discrete time steps. Capital letters with a subscript such as $A_t$ are regarded as random variables while a lowercase letter such as $a$ is a realisation of a random variable.

    For snake the action space $\actionspace = \{ \text{North}, \text{Sourth}, \text{East}, \text{West} \}$ but the state space $\statespace$ is not defined yet. We will present different state representations in the method.

    \subsubsection*{Dynamics}

    We will now formalize the reinforcement learning problem through the \textit{Markov Decision Process} (MDP), in particular the \textit{finite} MDP, by sticking to the limitations mentioned above. This formalization assumes that the problem satisfies the \textit{Markov property} which means that the one-step dynamics of the problem satisfies the following

    \begin{align*}
        Pr(S_{t + 1} = \nextstate, R_{t + 1} = r | S_0, A_0, R_1, \ldots, S_{t - 1}, A_{t - 1}, R_t, S_t, A_t) = Pr(S_{t + 1} = \nextstate, R_{t + 1} = r | S_t, A_t),
    \end{align*}

    which in words says that the future state-reward pair is independent of the past actions, states, and rewards given the current action-state pair. That is also equivalent to saying that the current state and action captures all the relevant information from the history. To simplify this formula we write it as

    \begin{align*}
        p(\nextstate, r | s, a) &=
        Pr(S_{t + 1} = \nextstate, R_{t + 1} = r | S_t = s, A_t = a)
    \end{align*}

    and it completely specifies the dynamics of the finite MDP, or equivalently the dynamics of the environment in which the agent is interacting. We can compute anything we might want to get from it such as the the \textit{state-transition probabilities} and the \textit{expected reward} for state-action pairs

    \begin{align*}
        p(\nextstate | s, a) &=
        \sum_r p(\nextstate, r | s, a), \\
        r(s, a) &=
        \expectation \left[ R_{t + 1} | S_t = s, A_t = a \right] =
        \sum_{r \in \rewardspace} r \sum_{\nextstate \in \statespace} p(\nextstate, r | s, a).
    \end{align*}

    In our case the state-transitions probabilities will most of the times be either 0 or 1 since the game will be deterministic in the sense that if the snake is facing north it will always move north, not south or any other direction, hence the next state is known for certain in most situations. The only random element is the placement of the apple so if the action results in the snake eating an apple the next state will not be known for certain.

    \subsubsection*{Policy}

    A \textit{policy} $\pi$ is a distribution over actions given states

    \begin{align*}
        \pi(a | s) &= Pr(A_t = a | S_t = s).
    \end{align*}

    It fully defines the behavior of an agent. It maps an observed state into an action and it could be stochastic in order to enforce \textit{exploration} which we will come back to later. A policy could be as simple as a lookup table or a complicated search process which further makes this framework very flexible. We have used the former approach.

    \subsubsection*{Reward}

    The \textit{reward signal} is an essential part to learning that will guide the agent to the target behavior. This implies that it is crucial that during the construction of your reinforcement learning problem the underlying reward function that generates the rewards should reflect what you want accomplished. As a simple example, in the game of chess you would consider winning as good and losing as bad and so the reward should reflect that by for instance give +1 reward for winning and -1 reward for losing.

    The sole goal of the agent is the maximize the total reward it receives in the long run, so we define the return $G_t$ as

    \begin{align*}
        G_t = R_{t + 1} + \gamma R_{t + 2} + \ldots = \sum_{k = 0}^{\infty} \gamma^k R_{t + k + 1},
    \end{align*}

    where $\gamma \in \left[0, 1 \right]$ is the \textit{discount factor} that determines how much weight the agent should put on immediate rewards versus future rewards. By adding the discount factor $\gamma < 1$ to $G_t$ we bound it so it does not become $\infty$. The goal of reinforcement learning is to maximize the return $G_t$ but since it is a random variable what we actually do care about is the \textit{state-value function}

    \begin{align*}
        v_{\pi}(s) &= \expectation_\pi \left[ G_t \bigg\rvert S_t = s \right]
        = \sum_a \pi(a | s) \sum_{\nextstate, r} p(\nextstate, r | s, a) \left[ r + \gamma v_\pi(\nextstate) \right],
    \end{align*}
    and the \textit{action-value function}

    \begin{align*}
        q_{\pi}(s, a) = \expectation_\pi \left[ G_t \bigg\rvert S_t = s, A_t = a \right]
        = \sum_{\nextstate, r} p(\nextstate, r | s, a) \left[ r + \gamma \sum_{\nextaction} \pi(\nextaction | \nextstate) q_\pi(\nextstate, \nextaction) \right].
    \end{align*}

    In words these functions expresses the expected return starting from state $s$ and then following policy $\pi$, and the expected return starting from state $s$ and performing action $a$ and then following policy $\pi$ respectively.

    These recursive equations are known as the Bellman equations discovered by Richard Bellman in the 1950s \cite{bellman1952theory}. What we are interested in is finding the Bellman optimality equations

    \begin{align*}
        v_{\ast}(s)
        &= \max_{\pi} v_{\pi}(s) \\
        &= \max_a \sum_{\nextstate, r} p(\nextstate, r | s, a) \left[ r + \gamma v_\ast(\nextstate) \right], \\
        \forall_s v_{\ast}(s) &\geq \forall_{s, \pi} v_{\pi}(s), \\
        q_{\ast}(s, a)
        &= \max_{\pi} q_{\pi}(s, a) \\
        &= \sum_{\nextstate, r} p(\nextstate, r | s, a) \left[ r + \gamma \max_{\nextaction} q_\ast(\nextstate, \nextaction) \right], \\
        \forall_s q_{\ast}(s, a) &\geq \forall_{s, \pi} q_{\pi}(s, a),
    \end{align*}

    and having those makes it trivial to find the optimal policy that maximizes the expected return which is the goal of the agent. It turns out that there always exists such a policy, possibly multiple such policies.

    The optimal policy $\pi_\ast$ is

    \begin{align*}
        \pi_\ast(a | s) &=
        \begin{cases}
            1, & \text{if}\ a = \displaystyle\argmax_{a} q_\ast(s, a) \\
            0, & \text{otherwise}
        \end{cases} \\
        &=
        \begin{cases}
            1, & \text{if}\ a = \displaystyle\argmax_{a} \sum_{\nextstate, r} p(\nextstate, r | s, a) \left[ r + \gamma v_\ast(\nextstate) \right] \\
            0, & \text{otherwise},
        \end{cases} \\
    \end{align*}

    and we can clearly see that is it easier to acquire the optimal policy if we have access to the optimal action-value function. The problem is that there are, in general, no closed form solution to finding these optimal value functions and therein lies the difficulty of reinforcement learning. After a summary of the Snake's MDP and an explanation of exploration we will present a few iterative solution methods for approximating these optimal value functions, the action-value function in particular, that have been used in this report.

    \subsubsection*{Snake MDP}

    We mentioned the term \textit{episodic task} and it refers to problems that have a natural notion of final time step which Snake naturally does, that is when the snake collides with something and dies. We denote such a state as the \textit{terminal state}.

    To summarize the Snake's MDP we have that the $\actionspace = \{ \text{North}, \text{South}, \text{East}, \text{West} \}$ and the state-transition probabilities are mostly either 0 or 1 with the exception of when the action makes the snake eat an apple which the environment will react to by placing a new apple on the grid uniformly at random. We have not yet defined $\statespace$ nor the reward function which we will come back to in the method.
    
    \subsubsection*{Exploration versus Exploitation}

    An important concept in reinforcement learning is the \textit{exploration-exploitation} trade-off. It refers to the problem that the agent does not know how it should behave to maximize the expected return and thus have to interact with the environment to learn by trial-and-error. This means that it has to explore the environment by interactions and observe their consequences and the reward signals. This information does not contain what the \enquote{right} action was supposed to be but the agent can still construct a policy from it. Exploiting means that the agent is using its current policy to determine the best action, i.e., it is greedy with respect to the state-value function (or action-value function). It can happen that the current policy is stuck in a local optima because the agent has not explored the search space thorough enough to build a good estimate of the performance of other actions.

    A simple technique to enforcing exploration is by training using $\epsilon$-greedy policy, see algorithm \ref{alg:epsilon_greedy_policy}, which takes the greedy action with probability $1 - \epsilon$ and a uniformly random action with probability $\epsilon$. This is the approach taken in this report but there are more sophisticated techniques to balance these two acts.

    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Require{State $s$, Action-value function $q$,
            Exploration rate $\epsilon \in [0, 1]$}
            \Statex
            \Function{EpsilonGreedyPolicy}{$s, q, \epsilon$}
            \Let{sample $u \sim U(0, 1)$}
            \If{$u < \epsilon$}
            \Let{sample $i \sim U(\{ 1, 2, \ldots, |\actionspace| \})$}
            \Let{$a = \actionspace_i$}
            \Else
            \Let{$a = \displaystyle\argmax_a q(s, a)$}
            \EndIf
            \State
            \Return{$a$}
            \EndFunction
        \end{algorithmic}
        \caption{$\epsilon$-Greedy Policy}
        \label{alg:epsilon_greedy_policy}
    \end{algorithm}

    A term that was mentioned previously was \textit{episodic task} which refers to problems that have a natural notion of final time step which Snake does, that is when the snake collides with something. We denote this state as the \textit{terminal state} which will follow with a reset of the game in the same starting state independently of how it ended.
    
    \subsection*{Temporal Difference Learning}

    \textit{Temporal Difference} (TD) learning is a learning methodology that learns directly from raw experience without a model of the environment's dynamics. It updates new estimates partially on previous estimates without waiting for the final outcome. The algorithms, or learning methods, that have been used in this paper are all based on TD learning and will be described below. Algorithm \ref{alg:td_learning} shows the general outline of how it works and what will differ between the algorithms is line 9, the update rule of $Q(S, A)$. We differentiate choosing the action, determined by our policy, from actually performing it so it becomes a two step process to take an action. We have used a tabular representation of the Q-function, i.e., we map a state-action pair to an action-value using a hash table.

    \begin{algorithm}[H]
        \begin{algorithmic}[1]
            \Require{Learning rate $\alpha \in [0, 1]$, Discount factor $\gamma \in [0, 1]$}
            \Statex
            \Function{TDLearning}{$\alpha, \gamma$}
            \Let{Initialize $\forall_{a \in \actionspace, s \in \statespace} Q(a, s)$ arbitrarily and $Q$(terminal state, $\cdot$) = 0}

            \For{each episode}
            \Let{Initialize $S$}
            \Let{Choose $A$ from $S$ using policy $\pi$ ($\epsilon$-greedy) derived from $Q$ }

            \While{$S$ is non-terminal state}
            \Let{Perform action $A$, observe state $S^\prime$ and reward $R$}
            \Let{Choose $A^\prime$ from $S^\prime$ using policy $\pi$ ($\epsilon$-greedy) derived from $Q$}
            \Let{Update $Q(S, A)$ based on $(S, A, R, S^\prime, A^\prime, \pi, \alpha, \gamma$)}
            \Let{$S = S^\prime$}
            \Let{$A = A^\prime$}
            \EndWhile
            \EndFor
            \State
            \Return{Q}
            \EndFunction
        \end{algorithmic}
        \caption{TD Learning Algorithm}
        \label{alg:td_learning}
    \end{algorithm}

    \subsubsection*{Q-Learning}

    \textit{Q-learning} is a widely used algorithm that was developed by Watkins \cite{watkins1989learning} and variations of it is heavily used today combined with the emergence of neural networks \cite{lange2010deep}, \cite{van2016deep}. The update rule is defined as

    \begin{align*}
        Q(S_t, A_t) &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma \max_a Q(S_{t + 1}, a) - Q(S_t, A_t)
        \right].
    \end{align*}

    It is a so called \textit{off-policy} method which is indicated by the $\max$ operation. That means it uses a greedy algorithm to estimate the return at the next step rather than following its current policy, therefore is off its own policy.

    \subsubsection*{Sarsa}

    \textit{Sarsa} on the other hand is an on-policy method for estimating the action-value function and its name refers to that we use the quintuple of events $(S_t, A_t, R_{t + 1}, S_{t + 1}, A_{t + 1})$, and it has the update rule

    \begin{align*}
        Q(S_t, A_t) &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma Q(S_{t + 1}, A_{t + 1}) - Q(S_t, A_t)
        \right].
    \end{align*}

    The difference betwen Sarsa and Q-learning is that it does not use the $\max$ operator and is rather using its current policy to estimate the next action-value. There is also a natural extension to Sarsa called \textit{Expected Sarsa} which takes the expected action-value estimate under the current policy. The update rule is defined as

    \begin{align*}
        Q(S_t, A_t) &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma \expectation \left[ Q(S_{t + 1}, A_{t + 1}) | S_{t + 1}
        \right] -
        Q(S_t, A_t)
        \right] \\
        &=
        Q(S_t , A_t) +
        \alpha \left[
        R_{t + 1} +
        \gamma \sum_a \pi (a | S_{t + 1}) Q(S_{t + 1}, a) -
        Q(S_t, A_t)
        \right].
    \end{align*}

    \subsection*{Other Learning Methods}

    There are many other learning methods that are not used in this report that have interesting properties. Methods such as those based on \textit{dynamic programming} which is an important optimization method, \textit{Monte Carlo}, and also extensions to those as well as to the algorithms presented above. We highly recommend the book written by Sutton and Barto \cite{sutton1998reinforcement} for more information.

\end{document}
