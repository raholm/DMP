\documentclass[result.tex]{subfiles}

\begin{document}

\section*{\centering Conclusion}

    We have looked at two fundamental building blocks of reinforcement learning, the reward function and the state representation, to investigate their importance to learning. We used a simple variation of the game Snake as our problem. We have shown that the reward function and the state representation are important in order for the agent to learn the objective. The reward function should reflect the object and the state requires that it provides useful information. The characteristics of the learning algorithms do seem to play a part in which reward functions and state representations that are most useful which should be taking into consideration during the design phase of your reinforcement learning problem.

    We used the algorithms as they were first discovered, but there have been many extensions to improve upon them. It would be interesting to see if the experiments give similar outcome with extensions such as double Q-learning \cite{hasselt2010double}, experience replay \cite{lin1992self}, and function approximations rather than tabular representation of the Q-function.

\end{document}
