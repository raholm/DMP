\documentclass[result.tex]{subfiles}

\begin{document}

    \section*{\centering Discussion}

    In this part we discuss the results that we have gotten from our two experiments and point out what we have learned. We discuss them in the order they were presented previously.

    \subsection*{Reward Results}

    The first thing we noticed from the reward experiment is that the reward functions create two clusters where those function within a cluster have similar performance. We can see that those that give a big negative penalty for border collision create one cluser and the other three functions create another cluster. For Q-learning the state representation seems to be important in determining which of the clusters perform better while that is not the case for Sarsa. That indicate that they do have different behavior. Already here we can see that the board state perform in general better than the directional state, we will look at it further in the analysis of the state experiment.

    Since the average game score is not even 100 this means that the agent does not eat a single apple every episode which may indicate that it has not actually learnt anything and is just randomly moving around and gets lucky. By watching the agent play the game we have seen that it usually navigates to the first food source but then commits suicide by colliding with the game border. On rare occasions when the next food gets located nearby it eats two of them before colliding with the game border. That was the main motivation for creating the reward functions that penalize that kind of behavior. For those we saw it learns to get food until it was long enough to kill itself by self-colliding so it does not really learn to play but rather how to kill itself as soon as possible.

    The results suggest that it is important that the reward function to reinforce desirable behavior. This is more obvious when looking in the appendix and compare states that do not give any reinforcement to those that do. This is further supported by the fact that the reward function and the game score are highly correlated as shown in table \ref{tab:reward_game_score_corr} which indicate that is it important that the reward function reflect the goal that we want the agent to achieve. This is further indicated by the example above about the agent that started killing itself by self-colliding when given large negative rewards for colliding with the game border. So penalizing or rewarding sub-goals, such as not colliding with the game border, may influence the agent's behavior in an undesirable manner.


    \subsection*{State Results}

    The first thing we looked at was to compare the regular states with the augmented ones. From figure \ref{fig:info_augmentation_board_state} it is obvious that the state should contain the game score for significant increase in performance. The board dimension do not seem to be important at all and perhaps the reason is that it is a static value, thus do not add anything in the long run. One problem may arise when including the game score variable given our tabular representation which is that as soon as the game score changes all information learned from another game score is wiped. This can also be true without the score if the state contain information about the length of the snake or positioning of the body.

    In figure \ref{fig:state_qlearning} and \ref{fig:state_sarsa} we can see that the algorithms behave slightly differently, especially with regards to the directional state. That state seems to have converged with poor performance for Q-learning but still show some potential in Sarsa. The reason behind this difference is perhaps because Sarsa is an on-policy algorithm and is able to take advantage of the directional information better than an off-policy algorithm such as Q-learning.

    The shortest path state has the best performance both for Q-learning and Sarsa. By watching its behavior in a simulator we could see that it actually performed decent in picking up 1-2 apples. Since this state do not change fundamentally whenever picking an apple except the game score part it may be that is suffers from the problem mentioned above. We can also see that it has not converged so longer training time is necessary to increase its performance.

    What is particular interesting is that the board state and the snake-food state have almost identical performance. Important to note is that the snake-food state is basically a sparse representation of the board state so they do contain the same information and the algorithms do not seem to be bothered by that at all. This indicates that it can be beneficial to find a more computational efficient representation that contain the same information without big losses in performance.

    Tables \ref{table:state_qlearning} and \ref{table:state_sarsa} give us some insight into the performance of the different states. They provide the same conclusions about the ranking in performance but interestingly enough, according to the standard deviation, the board and snake-food states are less variable. This may be because they contain the whole game state rather than some simplified representation. We can see that the random agent gets roughly one apple every 5th game and our models gets roughly one apple per game, except the directional state, which is not particular impressive.

    We can say, from the results, that the state representation is also very important for learning and their performance can vary depending on your choice of algorithm. It seems that these methods are stable enough to not be disturbed by constant information such as the board dimensions and that such information can be considered essentially useless. However, given the results we cannot claim that the models have learnt to play Snake.

\end{document}
